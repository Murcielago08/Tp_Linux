# Partie 2 : HA

![THE server](./img/the_server.jpg)

## Sommaire

- [Partie 2 : HA](#partie-2--ha)
  - [Sommaire](#sommaire)
- [I. Scaling serveur web](#i-scaling-serveur-web)
  - [1. Serveurs Web additionnels](#1-serveurs-web-additionnels)
  - [2. Loadbalancing vers les serveurs Web](#2-loadbalancing-vers-les-serveurs-web)
- [II. Scaling reverse proxy](#ii-scaling-reverse-proxy)
  - [0. Intro](#0-intro)
  - [1. Reverse proxy additionnel](#1-reverse-proxy-additionnel)
  - [2. Keepalived](#2-keepalived)
- [III. Scaling DB](#iii-scaling-db)
  - [0. Intro blablaaaa](#0-intro-blablaaaa)
  - [1. DB additionnelle](#1-db-additionnelle)
  - [2. Conf master slave](#2-conf-master-slave)

# I. Scaling serveur web

![Web Scale](./img/webscale.svg)

On va *scale* notre cluster actuel de serveur web :

- est-ce qu'on peut appeler √ßa un cluster s'il y a actuellement un seul serveur web ? On va dire que oui ! √®_√©
- *scale* √ßa veut dire augmenter le nombre de machines dans le cluster
- on va utiliser une r√©partition de charges entre les deux serveurs, gr√¢ce √† notre proxy NGINX

Au menu donc :

- cr√©er deux VMs suppl√©mentaires : `web2.tp5.b2` et `web3.tp5.b2`
- y faire une conf similaire √† celle de `web1.tp5.b2`
- adapter la conf du proxying sur `rp1.tp5.b2` pour qu'ils fasse du loadbalancing

## 1. Serveurs Web additionnels

‚ûú **Cr√©ez les deux VMs `web2.tp5.b2` et `web3.tp5.b2`**

- conf identique √† `web1.tp5.b2` sauf :
  - l'IP d'√©coute dans la conf Apache est diff√©rente
  - euh et c'est tout

| Node          | Adresse      | R√¥le                       |
| ------------- | ------------ | -------------------------- |
| `web1.tp5.b2` | `10.5.1.11`  | Serveur Web (Apache + PHP) |
| `web2.tp5.b2` | `10.5.1.12`  | Serveur Web (Apache + PHP) |
| `web3.tp5.b2` | `10.5.1.13`  | Serveur Web (Apache + PHP) |
| `rp1.tp5.b2`  | `10.5.1.111` | Reverse Proxy (NGINX)      |
| `db1.tp5.b2`  | `10.5.1.211` | DB (MariaDB)               |

## 2. Loadbalancing vers les serveurs Web

üåû **Modifier le fichier de conf d√©di√© au reverse-proxy**

- vous avez normalement un fichier d√©di√© √† la conf du proxying : `/etc/nginx/conf.d/app_nulle.conf`
- il contient juste un bloc `server { }` qui lui-m√™me contient un `proxy_pass http://web1.tp5.b2` normalemeeeeent
- adaptez votre fichier de conf comme suit :

```nginx
# on cr√©e un groupe de serveurs avec la clause 'upstream'
upstream app_nulle_servers {
    server web1.tp5.b2:80;
    server web2.tp5.b2:80;
    server web3.tp5.b2:80;
}

server {
    [...]
    # on proxy_pass vers ce groupe de serveurs
    proxy_pass http://app_nulle_servers;
    [...]
}
```

> Le loadbalancing effectu√© par d√©faut sur NGINX comme beaucoup d'autres outils est un b√™te *round-robin* ou quelque chose qui s'en rapproche : premi√®re requ√™te est envoy√©e au premier serveur, la deuxi√®me au deuxi√®me, troisi√®me au troisi√®me, et la quatri√®me de nouveau au premier. Chacun son tour quoi.

üåû **Vous pouvez reload NGINX pour que votre conf prenne effet**

üåû **Visitez l'app web depuis votre navigateur** (toujours avec `http://app_nulle.tp5.b2`)

- vous devriez constater que l'IP du serveur qui traite votre requ√™te change √† chaque requ√™te effectu√©e (press F5)
- vous pouvez aussi consulter les logs des diff√©rents services pour voir par o√π passe la requ√™te
  - logs NGINX pour voir qu'il a bien re√ßu la requ√™te
  - les logs des trois Apache pour voir qui l'a effectivement trait√©e
- je veux bien un `curl` depuis votre PC dans le compte-rendu.

> *Avec une vraie app, une conf aussi simpliste peut poser soucis. En effet, une vraie app utilise souvent un concept de "session" (genre tu te co √† Netflix, bah ensuite, peu importe le nombre de requ√™tes que tu fais, t'es toujours connect√© non ? C'est parce que le serveur g√®re ta "session"). On fait donc souvent au moins en sorte qu'un client donn√© estr toujours renvoy√© vers le m√™me serveur pour sa session.*

‚ûú **Pour suivre l'arriv√©e des logs en temps r√©el**

- soit vous consultez les logs avec `journalctl`
  - on peut donc `journalctl -xe -u httpd -f`
  - `-x` pour le mode *pager* : on se balade dans les logs avec les fl√®ches du clavier
  - `-e` comme *end* pour lire les logs depuis la fin : on consulte les derniers √©v√®nements
  - `-u` comme *unit* pour pr√©ciser sur quel unit√© (un service par exemple) on veut agir
  - `-f` comme *follow* qui permet de suivre en temps r√©el l'arriv√©e de nouveaux logs
- soit vous consultez les logs dans un fichier avec `cat` par exemple
  - on peut donc `tail -f <fichier>` pour suivre l'arriv√©e des logs en temps r√©el
  - les logs sont habituellement situ√©s dans `/var/log/`

> *Vous pouvez vous √©quiper avec des beaux terminaux pour avoir 4 shells ouverts devant vos yeux (un sur le reverse proxy, et un sur chaque serveur web) avec l'arriv√©e des logs en temps r√©el.*

# II. Scaling reverse proxy

## 0. Intro

![SPOF](./img/spof.jpeg)

C'est bien d'augmenter la taille du cluster de serveurs Web mais bon, y'a toujours qu'un seul proxy devant ! **On dit que le proxy dans ce contexte est un *SPOF* : Single Point Of Failure** (ou en fran√ßais : point unique de d√©faillance).

**Un *SPOF* dans une infra c'est quand y'a un moment o√π y'a qu'un seul chemin pour aller de A √† B.** Ici qu'un seul tuyau pour que le client atteigne le serveur Web : le seul reverse proxy.

**On va donc ajouter un deuxi√®me reverse proxy** qui pourra prendre la rel√®ve si le premier fail. Ici **pas de loadbalancing, mais plut√¥t de la tol√©rance de panne** donc.

**L'un des deux serveurs NGINX sera dit "actif" et l'autre "passif".** Si le serveur "actif" meurt, le "passif" prend le relais (et devient le serveur "actif").

Pour faire √ßa on va utilser...

- **une IP virtuelle**
  - ou VIP
  - une IP qui est port√©e par les deux serveurs en m√™me temps
- **cette VIP est mise en place par le protocole VRRP**
  - bah il sert √† √ßa
  - les deux machines se spamment avec le protocole VRRP pour conna√Ætre l'√©tat du cluster
  - si "l'actif" arr√™te de r√©pondre au "passif" en VRRP, il prend la rel√®ve et c'est lui qui r√©pondra √† la VIP
- **et on va setup √ßa avec la techno Keepalived**
  - un incontournable du monde Linux pour des setups HA

![Web Scale](./img/vip.svg)

## 1. Reverse proxy additionnel

‚ûú **M√™me musique : cr√©ez une nouvelle VM `rp2.tp5.b2`**

- conf identique √† `rp1.tp5.b2` √† part l'IP d'√©coute du proxy

| Node          | Adresse      | R√¥le                               |
| ------------- | ------------ | ---------------------------------- |
| `web1.tp5.b2` | `10.5.1.11`  | Serveur Web (Apache + PHP)         |
| `web2.tp5.b2` | `10.5.1.12`  | Serveur Web (Apache + PHP)         |
| `web3.tp5.b2` | `10.5.1.13`  | Serveur Web (Apache + PHP)         |
| `rp1.tp5.b2`  | `10.5.1.111` | Reverse Proxy (NGINX + Keepalived) |
| `rp2.tp5.b2`  | `10.5.1.112` | Reverse Proxy (NGINX + Keepalived) |
| VIP           | `10.5.1.110` | IP Virtuelle Keepalived            |
| `db1.tp5.b2`  | `10.5.1.211` | DB (MariaDB)                       |

## 2. Keepalived

üåû **Installez Keepalived sur les deux serveurs reverse proxy**

- effectuez une conf basique pour d√©signer `rp1.tp5.b2` comme le serveur actif
  - Keepalived utilise les mots
    - "MASTER" pour le serveur "actif"
    - et il d√©signe par "BACKUP" le serveur "passif"
  - il a un syst√®me de priorit√©
    - plus elle est haute, plus le serveur est prioritaire pour porter la VIP
  - autrement dit, vous choisissez qui est le serveur "actif" en le d√©signant comme "MASTER" avec une priorit√© haute
  - et l'inverse pour le slave
- je vous laisse libre de fouiller sur internet pour des exemples de conf, ce sera toujours pareil hein :
  - install paquet
  - conf
  - d√©marrer le service
- **la VIP doit √™tre `10.5.1.110`**

‚ûú **Modifier le fichier `hosts` de votre PC**

- `app_nulle.tp5.b2` doit d√©sormais pointer vers la VIP `10.5.1.110`

‚ûú **V√©rifier le bon fonctionnement**

- avec `ip a` vous devriez voir qui porte actuellement la VIP `10.5.1.110`
- faites un test en visitant l'app depuis votre navigateur (toujours `http://app_nulle.tp5.b2`)

üåû **J'ai dit de tester que √ßa marchait**

- crasher `rp1.tp5.b2`
- constater que `rp2.tp5.b2` est devenu "actif" (il est pass√© "MASTER" et il porte la VIP)
- ***bonus*** : un truc chouette √† faire ce serait une ptite boucle `for` ou un `watch`, qui lance des `curl` en boucle sur `http://app_nulle.tp5.b2` pour voir s'il y a une interruption de service ou non quand la VIP bascule
- ***bonus*** : vous pouvez aussi Wireshark entre les deux reverse proxy avant/pendant/apr√®s le crash, pour voir les paquets VRRP √©chang√©s

# III. Scaling DB

## 0. Intro blablaaaa

Et enfin, notre p'tite DB. On va ajouter un deuxi√®me serveur DB, l√† encore pour proposer de la HA, cette fois-ci au niveau du service de base de donn√©es.

‚ûú Ici √ßa va √™tre **du *"master/slave"*** encore comme setup, et cette fois j'attire votre attention sur **la complexit√© du probl√®me** :

- on peut pas juste loadbalance le trafic entre les deux DB, think about it
- si on le faisait, √ßa voudrait dire quoi ? On fait un `INSERT` et √ßa modifie qu'une seule des deux bases ?
- **l√† on parle pas juste de partager une IP entre les deux serveurs, mais de partager des donn√©es**
- d√®s qu'un `INSERT` est effectu√©, il faut que les donn√©es arrivent sur les deux serveurs de db
- il se passe quoi si on ins√®re une donn√©e en m√™me temps qui se retrouve avec le m√™me ID attribu√© ?
- **le setup le plus simple consiste donc en un *"master/slave"*** avec le *master* qui r√©ceptionne les requ√™tes, et le *slave* qui est une copie du *master*
- on peut acc√©der aux donn√©es sur le *slave*, mais uniquement en *read-only*

‚ûú **On dit donc que c'est un setup avec un cluster de base de donn√©es et de la r√©plication** ("r√©plication" c'est le fait que le *slave* r√©plique les donn√©es du *master*).

> *La d√©signation **"master/slave"** porte un peu √† d√©bat aujourd'hui. Compr√©hensible, on pourrait choisir d'autres mots ptet !*

![DB Scale](./img/db_scale.svg)

## 1. DB additionnelle

‚ûú **Vous connaissez la chanson : cr√©ez une nouvelle VM `db2.tp5.b2`**

- conf identique √† `db1.tp5.b2` √† part l'IP d'√©coute de la DB

| Node          | Adresse      | R√¥le                               |
| ------------- | ------------ | ---------------------------------- |
| `web1.tp5.b2` | `10.5.1.11`  | Serveur Web (Apache + PHP)         |
| `web2.tp5.b2` | `10.5.1.12`  | Serveur Web (Apache + PHP)         |
| `web3.tp5.b2` | `10.5.1.13`  | Serveur Web (Apache + PHP)         |
| `rp1.tp5.b2`  | `10.5.1.111` | Reverse Proxy (NGINX + Keepalived) |
| `rp2.tp5.b2`  | `10.5.1.112` | Reverse Proxy (NGINX + Keepalived) |
| VIP           | `10.5.1.110` | IP Virtuelle Keepalived            |
| `db1.tp5.b2`  | `10.5.1.211` | DB (MariaDB)                       |
| `db2.tp5.b2`  | `10.5.1.212` | DB (MariaDB)                       |

## 2. Conf master slave

Avec MariaDB, c'est natif, rien √† ajouter et on peut demander √† deux DB de former un cluster *master/slave*.

üåû **Configurer vos deux DBs pour former un cluster**

- je vous laisse trouver un ptit lien (ou plusieurs) poru configurer √ßa
- le [link server-world](https://www.server-world.info/en/note?os=Ubuntu_22.04) est pas mal, [la doc officielle](https://mariadb.com/kb/en/setting-up-replication/) aussi bien s√ªr
- pour constater que √ßa fonctionne bien :
  - vous pouvez v√©rifier l'√©tat du cluster avec des commandes SQL
  - vous pouvez aussi utiliser l'app et consulter les logs des deux bases de donn√©es

![DB fails](./img/dbfails.png)